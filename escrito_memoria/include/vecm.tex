En esta sección se presentarán los fundamentos matemáticos de dos modelos
usados en el estudio de series de tiempo, las condiciones que deben cumplirse y 
cómo serán aplicados a la predicción en el mercado FOREX. 

\section{Series de tiempo estacionarias}
Una serie de tiempo estrictamente estacionaria es una que el comportamiento
probabilístico de cada colección de valores $\{y_{t_1},y_{t_2},\dots,y_{t_L}\}$
es idéntico
a otro set desplazado en el tiempo, más preciso: \[ P\{y_{t_1} \leq
c_1,\dots,y_{t_L} \leq c_L\} = P\{y_{t_1+h} \leq c_1,\dots,y_{t_L+h}
\leq c_L\}
\quad \forall L \in \mathbb{N}, \forall h \in \mathbb{Z}\] \noindent donde
$c_1,\dots,c_L$ son constantes.

Este definición es muy fuerte y difícil de evaluar desde un set de datos único.
La versión débil de esta definición impone condiciones solo en los dos primeros
momentos.

Una serie de tiempo débilmente estacionaria es un proceso que su media,
varianza y autocovarianza no cambian en el tiempo:

\begin{eqnarray*}
E(Y_t) &=& \mu  \quad \forall t \in \mathbb{N} \\ E(Y^2_t) &=&
\sigma^2  \quad \forall t \in \mathbb{N} \\
\lambda(s,t)&=&\lambda(s+h,t+h) \quad \forall s,t \in \mathbb{N},
\forall h \in \mathbb{Z}
\end{eqnarray*}

\noindent con $\lambda(s,t) = E[(y_s-\mu)(y_t - \mu)]$.

\section{Vector AutoRegressive (VAR)}

VAR es un marco general que describe el comportamiento de un set $l$ de
variables endógenas como una combinación lineal de sus últimos $p$ valores.
Esas $l$ variables en el tiempo $t$ son representadas por el vetor $y_t$ como:

\begin{equation}
\label{eq:variables}
\mathbf{y}_t = 
\begin{bmatrix} y_{1,t} \\
y_{2,t} \\
\vdots \\
y_{l,t}
\end{bmatrix}
\end{equation}
\noindent Donde $y_{j,t}$ corresponde a la serie de tiempo $j$ evaluada en el
tiempo $t$.

El modelo VAR(p) describe el comportamiento de una variable dependiente en
términos de sus propios valores resagados y el de otras variables en el
sistema. El modelo con $p$ resagos se formula como el siguiente:

\begin{equation}
\label{eq:var}
 \mathbf{y}_t = \phi_1 \mathbf{y}_{t-1}  + \dots +   \phi_p\mathbf{y}_{t-p}
 + \mathbf{c} + \mathbf{\epsilon}_t, \qquad t=p+1, \dots, N
 \end{equation}

\noindent donde ${\phi_1,\dots,\phi_p}$ son $l \times l$ matrices de
coeficientes reales, $\mathbf{\epsilon}_{p+1},\dots,\mathbf{\epsilon}_N$ son
términos relacionados al error, $\mathbf{c}$ es un vector constante y $N$ es el
número total de muestras.

La matriz VAR tiene la siguiente forma:

\begin{equation}
 \label{eq:varmatrix}
               \underbrace{ \begin{bmatrix}
               \quad \\
               \mathbf{y}_{p+1} &
               \mathbf{y}_{p+2} &
               \dots & 
               \mathbf{y}_N \\
               \quad
               \end{bmatrix}}_{\substack{ \mathbf{B}\\l \times (N-p)}}   
= 
                \underbrace{\left[ 
                \begin{array}{ccccc}
                \quad & \quad & \quad & \quad & \quad \\
                \phi_1  & \phi_2 & \cdots & \phi_p & \mathbf{c} \\  
                \quad &\quad & \quad & \quad & \quad
               \end{array} 
               \right]}_{\substack{ \mathbf{X}\\ l \times (l \times p + 1 )}}
\underbrace{\begin{bmatrix}
   \mathbf{y}_{p}  & \mathbf{y}_{p+1} & \dots    & \mathbf{y}_{N-1}\\
   \mathbf{y}_{p-1}  & \mathbf{y}_{p} & \dots    & \mathbf{y}_{N-2}\\
   \vdots        & \vdots   & \ddots   & \vdots\\
   \mathbf{y}_{1} & \mathbf{y}_{2}   & \dots    & \mathbf{y}_{N-p}\\
   1 & 1   & \dots    & 1 
   \end{bmatrix}}_{\substack{ \mathbf{A}\\ (l\times p +1 )(N-p)}}
+
\underbrace{\begin{bmatrix}
                \quad \\
              \mathbf{\epsilon}_{p+1}  & 
              \mathbf{\epsilon}_{p+2}  & 
              \dots                & 
              \mathbf{\epsilon}_N \\
              \quad
             \end{bmatrix}}_{\substack{\mathbf{E}\\l \times (N-p) }} 
\end{equation}

\section{Integración y Cointegración}
Una serie de tiempo $\mathbf{y}$ es llamada integrada de orden $d$, si después
de diferenciar la variable $d$ veces, se obtiene una variable I(0) (proceso
estacionario):

\[
(1-L)^d \mathbf{y} \sim \text{I(0)}
\]
\noindent donde I(0) es una serie de tiempo esacionaria y $L$ es el operador de rezago, i.e,
\[
(1-L)\mathbf{y} = \Delta \mathbf{y}
\]
\noindent donde $\Delta \mathbf{y}(t) = \mathbf{y}(t)  -\mathbf{y}(t-1) \quad \forall t $.

Definamos $\mathbf{y}_t = \{\mathbf{y}^1, \dots, \mathbf{y}^l\}$ como un set de $l$ series de tiempo estacionarias 
I(1) que se dice que es cointegrada si un vector,
$\beta=[\beta(1),\dots,\beta(l)]^\intercal \in \mathbb{R}^l$  existe tal que la serie de tiempo, 

\begin{equation}
 \mathbf{Z}_t:= \beta^\intercal \mathbf{y}_t = \beta(1) \mathbf{y}^1 + \dots + \beta(l) \mathbf{y}^l \sim
  \text{I(0)}
  \end{equation}

En otras palabras, un set de variables I(1) se dice que son cointegradas si
existe una combinación lineal de ellas que es I(0).

El siguiente ejemplo~\cite{johansen1995} ayuda a ilustrar el significado de
$\beta$:

\textbf{Ejemplo:}

Si tenemos un proceso 2-dimensional $\mathbf{X}_t$, $t=1,\dots,T$ por:

\begin{eqnarray*}
\mathbf{X}_{1t} &=& \sum_{i=1}^t \epsilon_{1i} + \epsilon_{2t} \\
\mathbf{X}_{2t} &=& a \sum_{i=1}^t \epsilon_{1i} + \epsilon_{3t} 
\end{eqnarray*}

Como $\mathbf{X}_{1t}$ y $\mathbf{X}_{2t}$ son procesosI(1) y existe un vector
$\beta = [a -1]$ tal que:

\[
\beta^\intercal \mathbf{X}_t = a \mathbf{X}_{1t} -\mathbf{X}_{2t} = 
a\epsilon_{2t} - \epsilon_{3t} \sim \text{I(0)}
\]

entonces, ambos procesos se dicen que están cointegrados. Si agregamos un proceso I(0)
$\mathbf{X}_{3t} = \epsilon_{4t}$ encontramos que existe dos cointegraciones
vectors now: $\begin{bmatrix}a &-1& 0\end{bmatrix}$ y $\begin{bmatrix}0
&0&1\end{bmatrix}$ como:

\[
\beta^\intercal \mathbf{X}_t = 
\begin{bmatrix}
a & -1 & 0 \\
0 & 0 & 1
\end{bmatrix} 
\begin{bmatrix} 
\mathbf{X}_{1t} \\
\mathbf{X}_{2t} \\
\mathbf{X}_{3t}
\end{bmatrix} = 
\begin{bmatrix}
a\epsilon_{2t} - \epsilon_{3t} \\
\epsilon_{4t}
\end{bmatrix}
\]

Este ejemplo muestra como los vectores de cointegración describe la relación
estable entre los procesos a traves de relaciones lineales que son más
estacionarias que el proceso original.

\section{Vector Error Correlation (VEC)}

El modelo VEC es una forma especial de un modelo VAR para variables I(1) que
están también cointegradas. El modelo VEC es obtenido reemplazando
$\Delta \mathbf{y}_t = \mathbf{y}_t - \mathbf{y}_{t-1}$ en ecuación
(\ref{eq:var}). El modelo VEC es expresado en térmidos de diferencias,
tiene un término de correción de error y tiene la siguiente forma:

\begin{equation}
 \label{eq:vec}
  \Delta \mathbf{y}_t = 
   \underbrace{ \Omega\mathbf{y}_{t-1}}_\text{Término de corrección de error} + 
    \sum_{i=1}^{p-1}
    \phi_i^* \Delta \mathbf{y}_{t-i}  + \mathbf{c} + \mathbf{\epsilon}_t \quad ,
    \end{equation}

    \noindent donde las matrices de coeficientes $\Omega$ y $\phi_i^*$ son
    funciones de matrices $\phi_i$ (mostradas en la ecuación (\ref{eq:var})) de
    la siguiente forma:

    \begin{eqnarray*}
    \phi_i^* &: =& -\sum_{j=i+1}^{p} \phi_j \\
    \Omega &: =& -(\mathbb{I}-\phi_1-\dots-\phi_p) 
    \end{eqnarray*}

    La matriz $\Omega$ tiene las siguientes propiedades:
    \begin{itemize}
    \item Si $\Omega = 0$ no hay cointegración 
    \item Si $rank(\Omega)=l$ i.e full rank, entonces la serie de tiempo no es I(1) pero es estacionaria
    \item Si $rank(\Omega)=r,\quad 0 < r < l$ entonces, hay cointegración 
    y la matriz $\Omega$ se puede expresar como $\Omega =
    \alpha \beta^\intercal$, donde $\alpha$ y $\beta$ son $(l \times r)$
    matrices y $rank(\alpha)=rank(\beta)=r$.

    La columna de $\beta$ cointiene los vectores de cointegración y la fila de
    $\alpha$ corresponde a los vectores de ajuste. $\beta$ se obtiene a través
    el procedimiento de Johansen~\cite{johansen1988} mientras que $\alpha$ debe
    ser determinado como variable en el modelo VEC.

    Cabe observar que la factorización de la matriz $\Omega$ no es unica ya que por cada
    $r \times r$ matriz no singular $H$ tenemos:

\begin{eqnarray*}
\alpha \beta^\intercal &=& \alpha \mathbf{HH^{-1}} \beta^\intercal\\
&=&(\alpha\mathbf{H})(\beta(\mathbf{H}^{-1})^\intercal)^\intercal \\
&=& \alpha^*(\beta^*)^\intercal
\end{eqnarray*}

\noindent con $\alpha^* = \alpha\mathbf{H}$ y $\beta^* =
\beta(\mathbf{H}^{-1})^\intercal$.

Por lo tanto, para obtener valores únicos, son requeridas más restricciones
para el modelo.

\end{itemize}

Si existe cointegración, entonces la ecuación (\ref{eq:vec}) puede ser escrita:
\begin{equation}
 \label{eq:vecfull}
  \Delta \mathbf{y}_t = \alpha \beta^\intercal\mathbf{y}_{t-1} 
   + \sum_{i=1}^{p-1} \phi_i^*\Delta
   \mathbf{y}_{t-i}  + \mathbf{c} + \mathbf{\epsilon}_t \quad ,
   \end{equation}

   \noindent que es un modelo VAR pero para series de tiempo diferenciadas.

La forma matricial del modelo VEC es:

\begin{equation} \label{eq:vecmatrix}
\underbrace{
                \left[ \begin{array}{ccc}
               \quad & \mathbf{\Delta y}_{p+1} & \quad \\ 
               \quad & \mathbf{\Delta y}_{p+2} & \quad \\
               \quad & \vdots & \quad \\ 
               \quad & \vdots & \quad \\  
               \quad & \mathbf{\Delta y}_N & \quad 
               \end{array} \right]}_{\substack{\mathbf{B}\\ (N-p) \times l }} =
   \underbrace{\left[ 
    \begin{array}{cccccc}
     \quad & \quad & \quad & \quad & \quad & \quad \\
     \alpha & \phi_1^*  & \phi_2^* & \cdots & \phi_{p-1}^* & \mathbf{c} \\  
     \quad &\quad &\quad & \quad & \quad & \quad
     \end{array} 
      \right]}_{\substack{ \mathbf{X}\\ (l(p-1)+r+1) \times l}}
\underbrace{\begin{bmatrix} 
   \beta^\intercal \mathbf{y}_{p} & 
   \beta^\intercal \mathbf{y}_{p+1}&
   \cdots & \beta^\intercal \mathbf{y}_{N-1} \\
   \mathbf{\Delta y}_p & \mathbf{\Delta y}_{p+1} & \cdots 
   & \mathbf{\Delta y}_{N-1} \\ 
   \vdots & \vdots & \ddots & \vdots \\
   \mathbf{\Delta y}_2 & \mathbf{\Delta y}_{3} & \cdots 
   & \mathbf{\Delta y}_{N-p+1} \\ 
   \end{bmatrix}}_{\substack{\mathbf{A} \\ (N-p) \times (l \times (p-1)+r+1) }}
+
\underbrace{\begin{bmatrix}
              \quad &\mathbf{\epsilon}_{p+1} & \quad \\ 
              \quad &\vdots & \quad\\ 
              \quad & \vdots & \quad\\
              \quad & \vdots & \quad\\
              \quad &\mathbf{\epsilon}_N & \quad
             \end{bmatrix}}_{\substack{\mathbf{E}\\ (N-p) \times l }} 
\end{equation}

Los parámetros de los modelos VAR y VEC mostrados en la ecuación
(\ref{eq:varmatrix}) y (\ref{eq:vecmatrix}) pueden ser resueltos usando
técnicas de regresion estándar, como mínimos cuadrados ordinaria (OLS por si
sigla en inglés). Sin embargo, la matrix $\mathbf{A}$ es usualmente deficientes
de rank y la solución de OLS no puede ser encontrada.  El método de Ridge
regression (RR) es comunmente usado en vez de OLS cuando las matrices son mal
condicionadas o deficientes de rango, ya que tiene mejoras en la generalización
en la solución del problema.

\section{Método de Mínimos Cuadrados}
Mínimos Cuadrados, conocido en la literatura en inglés como Ordinary Least Squares (OLS), 
es un método aplicable a la solución de sistemas de ecuaciones lineales. OLS consiste en minimzar
la suma de los errores cuadrados que equivale a minimzar la siguiente expresión:

\begin{equation}
\label{eq:regressionproblem}
\underset{\mathbf{X}}{\text{min}} \quad \| \mathbf{A}\mathbf{\mathbf{X}} - \mathbf{B} \|_2^2
\end{equation}

\noindent para la cual la solución conocida es $\hat{\mathbf{X}}$:

\begin{equation}
\label{eq:MP}
\hat{\mathbf{X}}=\mathbf{A}^{\!\!+}\,\mathbf{B}
\end{equation}

\noindent donde $\mathbf{A}^{\!\!+}$ es la pseudo-inversa de Moore-Penrose que se puede escribir como:

\begin{equation}
\label{eq:pseudoinverse}
\mathbf{A}^{\!\!+}= (\mathbf{A}^{\!\!\top} \mathbf{A})^{-1}\mathbf{A}^{\!\!\top} \, .
\end{equation}

Sin embargo, cuando $\mathbf{A}$ no es full rank, i.e 
$rank(\mathbf{A})=k <  n \leq m$, $\mathbf{A}^\top \mathbf{A}$ es siempre singular
y la ecuación~(\ref{eq:pseudoinverse}) no puede ser usada. Más generalmente,
la pseudo-inversa es mejor calculada usando la descomposición de valor singular compacta (SVD)
de $\mathbf{A}$:

\begin{equation}
    \label{eq:compactsvd}
    \underset{m \times n}{\mathbf{A}}=
    \underset{m \times k}{\mathbf{U_1}} \enskip
    \underset{k \times k}{\Sigma_1} \enskip
    \underset{k \times n}{\mathbf{V}_1^{\top}} \, ,
\end{equation}

\noindent por consiguiente

\begin{equation}
\label{eq:pseudoinversesvd}
\mathbf{A}^{\!\!+} = \mathbf{V}_1 \Sigma_1^{-1} \mathbf{U}_1^\top \, .
\end{equation}


\textbf{Demostración}\quad

Dado que $\mathbf{A}$ es singular el problema mostrado en la ecuación~(\ref{eq:regressionproblem}) 
no tiene solución, la norma mínima dada por la ecuación ~(\ref{eq:MP}) se obtiene resolviendo
el problema equivalente:

\begin{equation*}
\label{eq:proyectorsol}
\mathbf{A \hat{X} = PB} 
\end{equation*}

\noindent donde $\mathbf{P=U_1 U_1^\top}$ es la proyección en Col($\mathbf{A}$).

Dado que $\mathbf{V} = [\underset{(n \times k)}{\mathbf{V_1}} |
\underset{(n \times k)}{\mathbf{V_2}}]$ y $\mathbf{V_1^\top V_2 =
0}$ podemos expresar $\mathbf{\hat{X}} = \mathbf{V_1 x_1 + V_2 x_2}$
con $\mathbf{x_2=0}$ porque $\mathbf{\hat{X}}$ vive en el
$\text{Row}(\mathbf{A})$ dado por $\mathbf{V_1}$, entonces tenemos:

\begin{eqnarray*}
\mathbf{A \hat{X}} &=& \mathbf{PB} \\
\mathbf{U_1 \Sigma_1 V_1^\top \hat{X}} &=& \mathbf{U_1 U_1^\top B} \\
\mathbf{ V_1^\top \hat{X}} &=&  \mathbf{\Sigma_1^{-1} U_1^\top B} \\ 
\mathbf{ V_1^\top V_1 x_1} &=& \mathbf{\Sigma_1^{-1}
U_1^\top B} \\
\mathbf{x_1}&=& \mathbf{\Sigma_1^{-1} U_1^\top B}
\end{eqnarray*}

\noindent desde este resultados podemos obtener $\mathbf{\hat{X}}$ y
por lo tanto la expresión de la pseudo-inversa:

\begin{eqnarray*}
\mathbf{\hat{X}} &=& \mathbf{V_1 x_1} \\
                &=& \mathbf{V_1 \Sigma_1^{-1} U_1^\top B} \\
\mathbf{A^+} &=& \mathbf{V_1 \Sigma_1^{-1} U_1^\top} \, .
\end{eqnarray*}


